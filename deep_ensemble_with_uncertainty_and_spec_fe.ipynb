{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4bcb4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import copy\n",
    "import gc\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "from typing import List, Tuple, Union\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dab316",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a770f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import median_absolute_error, r2_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import Binarizer, KBinsDiscretizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import tensor_util\n",
    "from tensorflow.python.keras.utils import losses_utils, tf_utils\n",
    "from tensorflow.python.ops.losses import util as tf_losses_util\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_probability as tfp\n",
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b2fa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from assessment import calc_uncertainty_regection_curve, f_beta_metrics\n",
    "from uncertainty import ensemble_uncertainties_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac15a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunctionWrapper(tf.keras.losses.Loss):\n",
    "    def __init__(self,\n",
    "                 fn,\n",
    "                 reduction=losses_utils.ReductionV2.AUTO,\n",
    "                 name=None,\n",
    "                 **kwargs):\n",
    "        super(LossFunctionWrapper, self).__init__(reduction=reduction, name=name)\n",
    "        self.fn = fn\n",
    "        self._fn_kwargs = kwargs\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        if tensor_util.is_tensor(y_pred) and tensor_util.is_tensor(y_true):\n",
    "            y_pred, y_true = tf_losses_util.squeeze_or_expand_dimensions(y_pred, y_true)\n",
    "        return self.fn(y_true, y_pred, **self._fn_kwargs)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {}\n",
    "        for k, v in six.iteritems(self._fn_kwargs):\n",
    "            config[k] = tf.keras.backend.eval(v) if tf_utils.is_tensor_or_variable(v) \\\n",
    "                else v\n",
    "        base_config = super(LossFunctionWrapper, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2013cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def npairs_loss(labels, feature_vectors):\n",
    "    feature_vectors_normalized = tf.math.l2_normalize(feature_vectors, axis=1)\n",
    "    logits = tf.divide(\n",
    "        tf.matmul(\n",
    "            feature_vectors_normalized, tf.transpose(feature_vectors_normalized)\n",
    "        ),\n",
    "        0.5  # temperature\n",
    "    )\n",
    "    return tfa.losses.npairs_loss(tf.squeeze(labels), logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fcc991",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NPairsLoss(LossFunctionWrapper):\n",
    "    def __init__(self, reduction=losses_utils.ReductionV2.AUTO,\n",
    "                 name='m_pairs_loss'):\n",
    "        super(NPairsLoss, self).__init__(npairs_loss, name=name,\n",
    "                                         reduction=reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c8db42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_preprocessor(X: np.ndarray, colnames: List[str]) -> Pipeline:\n",
    "    X_ = Pipeline(steps=[\n",
    "        (\n",
    "            'imputer', SimpleImputer(\n",
    "                missing_values=np.nan, strategy='constant',\n",
    "                fill_value=-1.0\n",
    "            )\n",
    "        ),\n",
    "        (\n",
    "            'scaler',\n",
    "            MinMaxScaler()\n",
    "        )\n",
    "    ]).fit_transform(X)\n",
    "    X_ = np.rint(X_ * 100000.0).astype(np.int32)\n",
    "    binary_features = dict()\n",
    "    categorical_features = dict()\n",
    "    removed_features = []\n",
    "    for col_idx in range(X.shape[1]):\n",
    "        values = set(X_[:, col_idx].tolist())\n",
    "        print(f'Column {col_idx} \"{colnames[col_idx]}\" has ' \\\n",
    "              f'{len(values)} unique values.')\n",
    "        if len(values) > 1:\n",
    "            if len(values) < 3:\n",
    "                binary_features[col_idx] = np.min(X[:, col_idx])\n",
    "            else:\n",
    "                categorical_features[col_idx] = len(values)\n",
    "        else:\n",
    "            removed_features.append(col_idx)\n",
    "        del values\n",
    "    del X_\n",
    "    all_features = set(range(X.shape[1]))\n",
    "    useful_features = sorted(list(all_features - set(removed_features)))\n",
    "    if len(useful_features) == 0:\n",
    "        raise ValueError('Training inputs are bad. All features are removed.')\n",
    "    print(f'There are {X.shape[1]} features.')\n",
    "    if len(removed_features) > 0:\n",
    "        print(f'These features will be removed: ' \\\n",
    "              f'{[colnames[col_idx] for col_idx in removed_features]}.')\n",
    "    transformers = []\n",
    "    if (len(categorical_features) > 0) and (len(binary_features) > 0):\n",
    "        print(f'There are {len(categorical_features)} categorical ' \\\n",
    "              f'features and {len(binary_features)} binary features.')\n",
    "    elif len(categorical_features) > 0:\n",
    "        print(f'There are {len(categorical_features)} categorical features.')\n",
    "    else:\n",
    "        print(f'There are {len(binary_features)} binary features.')\n",
    "    for col_idx in categorical_features:\n",
    "        n_unique_values = categorical_features[col_idx]\n",
    "        transformers.append(\n",
    "            (\n",
    "                colnames[col_idx],\n",
    "                KBinsDiscretizer(\n",
    "                    n_bins=min(max(n_unique_values // 3, 3), 256),\n",
    "                    encode='ordinal',\n",
    "                    strategy=('quantile' if n_unique_values > 50 else 'kmeans')\n",
    "                ),\n",
    "                (col_idx,)\n",
    "            )\n",
    "        )\n",
    "    for col_idx in binary_features:\n",
    "        transformers.append(\n",
    "            (\n",
    "                colnames[col_idx],\n",
    "                Binarizer(threshold=0.0),\n",
    "                (col_idx,)\n",
    "            )\n",
    "        )\n",
    "    preprocessor = Pipeline(steps=[\n",
    "        (\n",
    "            'imputer', SimpleImputer(\n",
    "                missing_values=np.nan, strategy='constant',\n",
    "                fill_value=-1.0\n",
    "            )\n",
    "        ),\n",
    "        (\n",
    "            'minmax_scaler',\n",
    "            MinMaxScaler()\n",
    "        ),\n",
    "        (\n",
    "            'composite_transformer', ColumnTransformer(\n",
    "                transformers=transformers,\n",
    "                sparse_threshold=0.0,\n",
    "                n_jobs=1\n",
    "            )\n",
    "        ),\n",
    "        (\n",
    "            'selector',\n",
    "            VarianceThreshold()\n",
    "        ),\n",
    "        (\n",
    "            'standard_scaler',\n",
    "            StandardScaler(with_mean=True, with_std=True)\n",
    "        ),\n",
    "        (\n",
    "            'pca',\n",
    "            PCA(random_state=42)\n",
    "        )\n",
    "    ])\n",
    "    return preprocessor.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c910eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dimensions_of_data(features: np.ndarray) -> np.ndarray:\n",
    "    preprocessed_features = Pipeline(\n",
    "        steps=[\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('pca', PCA(n_components=features.shape[1] // 3,\n",
    "                        random_state=42))\n",
    "        ]\n",
    "    ).fit_transform(features)\n",
    "    print('Features are preprocessed.')\n",
    "    reduced_features = umap.UMAP(\n",
    "        low_memory=False,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=True\n",
    "    ).fit_transform(preprocessed_features)\n",
    "    print('Feature space is reduced.')\n",
    "    del preprocessed_features\n",
    "    return reduced_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8000144c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_temperature(features: np.ndarray, targets: np.ndarray,\n",
    "                     title: str='', figure_id: int=0):\n",
    "    if features.shape[0] != targets.shape[0]:\n",
    "        err_msg = f'Features do not correspond to targets! ' \\\n",
    "                  f'{features.shape[0]} != {targets.shape[0]}'\n",
    "        raise ValueError(err_msg)\n",
    "    if len(features.shape) != 2:\n",
    "        err_msg = f'Features are wrong! Expected 2-D array, got ' \\\n",
    "                  f'{len(features.shape)}-D one.'\n",
    "        raise ValueError(err_msg)\n",
    "    if features.shape[1] != 2:\n",
    "        err_msg = f'Features are wrong! Expected number of ' \\\n",
    "                  f'columns is 2, got {features.shape[1]}.'\n",
    "        raise ValueError(err_msg)\n",
    "    if len(targets.shape) != 1:\n",
    "        err_msg = f'Targets are wrong! Expected 1-D array, got ' \\\n",
    "                  f'{len(targets.shape)}-D one.'\n",
    "        raise ValueError(err_msg)\n",
    "    sorted_targets = sorted(targets.tolist())\n",
    "    n_percentile2 = max(int(round(0.01 * len(sorted_targets))), 1)\n",
    "    min_target = sorted_targets[n_percentile2]\n",
    "    max_target = sorted_targets[-n_percentile2]\n",
    "    del sorted_targets\n",
    "    clipped_targets = np.empty(targets.shape, dtype=np.float64)\n",
    "    for sample_idx in range(targets.shape[0]):\n",
    "        if targets[sample_idx] < min_target:\n",
    "            clipped_targets[sample_idx] = min_target\n",
    "        elif targets[sample_idx] > max_target:\n",
    "            clipped_targets[sample_idx] = max_target\n",
    "        else:\n",
    "            clipped_targets[sample_idx] = targets[sample_idx]\n",
    "    temperature_colors = clipped_targets.tolist()\n",
    "    temperature_norm = Normalize(vmin=np.min(temperature_colors),\n",
    "                                 vmax=np.max(temperature_colors))\n",
    "    fig = plt.figure(figure_id, figsize=(11, 11))\n",
    "    plt.scatter(x=features[:, 0], y=features[:, 1],\n",
    "                marker='o', cmap=plt.cm.get_cmap(\"jet\"), c=temperature_colors,\n",
    "                norm=temperature_norm)\n",
    "    if len(title) > 0:\n",
    "        plt.title(f'UMAP projections of weather data {title} (temperature)')\n",
    "    else:\n",
    "        plt.title(f'UMAP projections of weather data (temperature)')\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd4887b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataset(y: np.ndarray) -> List[int]:\n",
    "    all_values = sorted(y.tolist())\n",
    "    n = len(all_values)\n",
    "    if n <= 10000:\n",
    "        err_msg = f'y is wrong! Expected length of y is greater than 10000, ' \\\n",
    "                  f'but got {n}.'\n",
    "        raise ValueError(err_msg)\n",
    "    y001 = all_values[int(round((n - 1) * 0.001))]\n",
    "    y999 = all_values[int(round((n - 1) * 0.999))]\n",
    "    del all_values\n",
    "    filtered_indices = list(filter(\n",
    "        lambda idx: (y[idx] > y001) and (y[idx] < y999),\n",
    "        range(n)\n",
    "    ))\n",
    "    return filtered_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6b3941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_neural_network(input_size: int, layer_size: int, n_layers: int,\n",
    "                         dropout_rate: float, scale_coeff: float,\n",
    "                         nn_name: str) -> tf.keras.Model:\n",
    "    feature_vector = tf.keras.layers.Input(\n",
    "        shape=(input_size,), dtype=tf.float32,\n",
    "        name=f'{nn_name}_feature_vector'\n",
    "    )\n",
    "    outputs = []\n",
    "    hidden_layer = tf.keras.layers.AlphaDropout(\n",
    "        rate=dropout_rate,\n",
    "        seed=random.randint(0, 2147483647),\n",
    "        name=f'{nn_name}_dropout1'\n",
    "    )(feature_vector)\n",
    "    for layer_idx in range(1, (2 * n_layers) // 3 + 1):\n",
    "        try:\n",
    "            kernel_initializer = tf.keras.initializers.LecunNormal(\n",
    "                seed=random.randint(0, 2147483647)\n",
    "            )\n",
    "        except:\n",
    "            kernel_initializer = tf.compat.v1.keras.initializers.lecun_normal(\n",
    "                seed=random.randint(0, 2147483647)\n",
    "            )\n",
    "        hidden_layer = tf.keras.layers.Dense(\n",
    "            units=layer_size,\n",
    "            activation='selu',\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            bias_initializer='zeros',\n",
    "            name=f'{nn_name}_dense{layer_idx}'\n",
    "        )(hidden_layer)\n",
    "        hidden_layer = tf.keras.layers.AlphaDropout(\n",
    "            rate=dropout_rate,\n",
    "            seed=random.randint(0, 2147483647),\n",
    "            name=f'{nn_name}_dropout{layer_idx + 1}'\n",
    "        )(hidden_layer)\n",
    "    try:\n",
    "        kernel_initializer = tf.keras.initializers.LecunNormal(\n",
    "            seed=random.randint(0, 2147483647)\n",
    "        )\n",
    "    except:\n",
    "        kernel_initializer = tf.compat.v1.keras.initializers.lecun_normal(\n",
    "            seed=random.randint(0, 2147483647)\n",
    "        )\n",
    "    projection_layer = tf.keras.layers.Dense(\n",
    "        units=50,\n",
    "        activation=None,\n",
    "        use_bias=False,\n",
    "        kernel_initializer=kernel_initializer,\n",
    "        name=f'{nn_name}_projection'\n",
    "    )(hidden_layer)\n",
    "    for layer_idx in range((2 * n_layers) // 3 + 1, n_layers + 1):\n",
    "        try:\n",
    "            kernel_initializer = tf.keras.initializers.LecunNormal(\n",
    "                seed=random.randint(0, 2147483647)\n",
    "            )\n",
    "        except:\n",
    "            kernel_initializer = tf.compat.v1.keras.initializers.lecun_normal(\n",
    "                seed=random.randint(0, 2147483647)\n",
    "            )\n",
    "        hidden_layer = tf.keras.layers.Dense(\n",
    "            units=layer_size,\n",
    "            activation='selu',\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            bias_initializer='zeros',\n",
    "            name=f'{nn_name}_dense{layer_idx}'\n",
    "        )(hidden_layer)\n",
    "        hidden_layer = tf.keras.layers.AlphaDropout(\n",
    "            rate=dropout_rate,\n",
    "            seed=random.randint(0, 2147483647),\n",
    "            name=f'{nn_name}_dropout{layer_idx + 1}'\n",
    "        )(hidden_layer)\n",
    "    try:\n",
    "        kernel_initializer = tf.keras.initializers.LecunNormal(\n",
    "            seed=random.randint(0, 2147483647)\n",
    "        )\n",
    "    except:\n",
    "        kernel_initializer = tf.compat.v1.keras.initializers.lecun_normal(\n",
    "            seed=random.randint(0, 2147483647)\n",
    "        )\n",
    "    output_layer = tf.keras.layers.Dense(\n",
    "        units=2,\n",
    "        activation=None,\n",
    "        use_bias=False,\n",
    "        kernel_initializer=kernel_initializer,\n",
    "        name=f'{nn_name}_output'\n",
    "    )(hidden_layer)\n",
    "    bayesian_layer = tfp.layers.DistributionLambda(\n",
    "        lambda t: tfp.distributions.Normal(\n",
    "            loc=t[..., :1],\n",
    "            scale=1e-6 + tf.math.softplus((1.0 / scale_coeff) * t[..., 1:])\n",
    "        ),\n",
    "        name=f'{nn_name}_distribution'\n",
    "    )(output_layer)\n",
    "    neural_network = tf.keras.Model(\n",
    "        inputs=feature_vector,\n",
    "        outputs=[bayesian_layer, projection_layer],\n",
    "        name=nn_name\n",
    "    )\n",
    "    negloglik = lambda y, rv_y: -rv_y.log_prob(y)\n",
    "    radam = tfa.optimizers.RectifiedAdam(learning_rate=3e-4)\n",
    "    ranger = tfa.optimizers.Lookahead(radam, sync_period=6, slow_step_size=0.5)\n",
    "    losses = {\n",
    "        f'{nn_name}_distribution': negloglik,\n",
    "        f'{nn_name}_projection': NPairsLoss()\n",
    "    }\n",
    "    loss_weights = {\n",
    "        f'{nn_name}_distribution': 1.0,\n",
    "        f'{nn_name}_projection': 0.5\n",
    "    }\n",
    "    metrics = {\n",
    "        f'{nn_name}_distribution': [\n",
    "            tf.keras.metrics.MeanAbsoluteError()\n",
    "        ]\n",
    "    }\n",
    "    neural_network.compile(\n",
    "        optimizer=ranger,\n",
    "        loss=losses,\n",
    "        loss_weights=loss_weights,\n",
    "        metrics=metrics\n",
    "    )\n",
    "    return neural_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a07e55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_training_process(history: tf.keras.callbacks.History, metric_name: str,\n",
    "                          figure_id: int=1, comment: str=''):\n",
    "    val_metric_name = 'val_' + metric_name\n",
    "    if metric_name not in history.history:\n",
    "        err_msg = f'The metric \"{metric_name}\" is not found! Available metrics are: ' \\\n",
    "                  f'{list(history.history.keys())}.'\n",
    "        raise ValueError(err_msg)\n",
    "    plt.figure(figure_id, figsize=(5, 5))\n",
    "    interesting_metric = history.history[metric_name]\n",
    "    plt.plot(list(range(len(interesting_metric))), interesting_metric,\n",
    "             label=f'Training {metric_name}')\n",
    "    if val_metric_name in history.history:\n",
    "        interesting_val_metric = history.history[val_metric_name]\n",
    "        assert len(interesting_metric) == len(interesting_val_metric)\n",
    "        plt.plot(list(range(len(interesting_val_metric))),\n",
    "                 interesting_val_metric,\n",
    "                 label=f'Validation {metric_name}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(metric_name)\n",
    "    if len(comment) > 0:\n",
    "        plt.title(f'Training process of {comment}')\n",
    "    else:\n",
    "        plt.title('Training process')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af0af98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_single_nn(input_data: np.ndarray, model_for_prediction: tf.keras.Model,\n",
    "                           batch_size: int, output_scaler: StandardScaler) \\\n",
    "        -> Tuple[np.ndarray, np.ndarray]:\n",
    "    if len(input_data.shape) != 2:\n",
    "        err_msg = f'The `input_data` argument is wrong! Expected 2-D array, ' \\\n",
    "                  f'got {len(input_data.shape)}-D one!'\n",
    "        raise ValueError(err_msg)\n",
    "    n_batches = int(np.ceil(input_data.shape[0] / float(batch_size)))\n",
    "    pred_mean = []\n",
    "    pred_std = []\n",
    "    for batch_idx in range(n_batches):\n",
    "        batch_start = batch_idx * batch_size\n",
    "        batch_end = min(input_data.shape[0], batch_start + batch_size)\n",
    "        instant_predictions = model_for_prediction(input_data[batch_start:batch_end])[0]\n",
    "        if not isinstance(instant_predictions, tfp.distributions.Distribution):\n",
    "            err_msg = f'Minibatch {batch_idx}: predictions are wrong! ' \\\n",
    "                      f'Expected tfp.distributions.Distribution, ' \\\n",
    "                      f'got {type(instant_predictions)}.'\n",
    "            raise ValueError(err_msg)\n",
    "        instant_mean = instant_predictions.mean()\n",
    "        instant_std = instant_predictions.stddev()\n",
    "        del instant_predictions\n",
    "        if not isinstance(instant_mean, np.ndarray):\n",
    "            instant_mean = instant_mean.numpy()\n",
    "        if not isinstance(instant_std, np.ndarray):\n",
    "            instant_std = instant_std.numpy()\n",
    "        instant_mean = instant_mean.astype(np.float64).flatten()\n",
    "        instant_std = instant_std.astype(np.float64).flatten()\n",
    "        pred_mean.append(instant_mean)\n",
    "        pred_std.append(instant_std)\n",
    "        del instant_mean, instant_std\n",
    "    pred_mean = np.concatenate(pred_mean)\n",
    "    pred_std = np.concatenate(pred_std)\n",
    "    pred_mean = output_scaler.inverse_transform(\n",
    "        pred_mean.reshape((input_data.shape[0], 1))\n",
    "    ).flatten()\n",
    "    pred_std *= output_scaler.scale_[0]\n",
    "    return pred_mean, pred_std * pred_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a29aec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_single_nn(pred_means: np.ndarray, pred_vars: np.ndarray,\n",
    "                       true_outputs: np.ndarray) -> float:\n",
    "    if len(pred_means.shape) != 1:\n",
    "        err_msg = f'The `pred_means` argument is wrong! Expected 1-D array, ' \\\n",
    "                  f'got {len(pred_means.shape)}-D one.'\n",
    "        raise ValueError(err_msg)\n",
    "    if len(pred_vars.shape) != 1:\n",
    "        err_msg = f'The `pred_vars` argument is wrong! Expected 1-D array, ' \\\n",
    "                  f'got {len(pred_vars.shape)}-D one.'\n",
    "        raise ValueError(err_msg)\n",
    "    if len(true_outputs.shape) != 1:\n",
    "        err_msg = f'The `true_outputs` argument is wrong! Expected 1-D array, ' \\\n",
    "                  f'got {len(true_outputs.shape)}-D one.'\n",
    "        raise ValueError(err_msg)\n",
    "    n_test_samples = true_outputs.shape[0]\n",
    "    if n_test_samples < 5:\n",
    "        raise ValueError(f'Number of test samples = {n_test_samples} is too small!')\n",
    "    if n_test_samples != pred_means.shape[0]:\n",
    "        err_msg = f'The `pred_means` does not correspond to the `true_outputs`! ' \\\n",
    "                  f'{pred_means.shape[0]} != {n_test_samples}'\n",
    "        raise ValueError(err_msg)\n",
    "    if n_test_samples != pred_vars.shape[0]:\n",
    "        err_msg = f'The `pred_vars` does not correspond to the `true_outputs`! ' \\\n",
    "                  f'{pred_vars.shape[0]} != {n_test_samples}'\n",
    "        raise ValueError(err_msg)\n",
    "    \n",
    "    all_preds_ = np.empty((1, n_test_samples, 2), dtype=np.float32)\n",
    "    all_preds_[0, :, 0] = pred_means\n",
    "    all_preds_[0, :, 1] = pred_vars\n",
    "    all_uncertainty_ = ensemble_uncertainties_regression(all_preds_)\n",
    "    uncertainties = all_uncertainty_['tvar']\n",
    "    del all_preds_, all_uncertainty_\n",
    "    \n",
    "    errors = (pred_means - true_outputs) ** 2\n",
    "    rejection_mse_ = calc_uncertainty_regection_curve(errors, uncertainties)\n",
    "    return np.mean(rejection_mse_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3369e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_by_ensemble(input_data: np.ndarray,\n",
    "                        preprocessing: Pipeline,\n",
    "                        ensemble: List[tf.keras.Model],\n",
    "                        postprocessing: List[StandardScaler],\n",
    "                        minibatch: int) -> np.ndarray:\n",
    "    num_samples = input_data.shape[0]\n",
    "    ensemble_size = len(postprocessing)\n",
    "    if ensemble_size != len(ensemble):\n",
    "        err_msg = f'Ensemble of preprocessors does not correspond to ' \\\n",
    "                  f'ensemble of models! {ensemble_size} != {len(ensemble)}'\n",
    "        raise ValueError(err_msg)\n",
    "    predictions_of_ensemble = np.empty((ensemble_size, num_samples, 2),\n",
    "                                       dtype=np.float64)\n",
    "    X = preprocessing.transform(input_data).astype(np.float32)\n",
    "    for model_idx, (cur_model, post_) in enumerate(zip(ensemble, postprocessing)):\n",
    "        y_mean, y_var = predict_with_single_nn(\n",
    "            input_data=X,\n",
    "            model_for_prediction=cur_model,\n",
    "            output_scaler=post_,\n",
    "            batch_size=minibatch\n",
    "        )\n",
    "        predictions_of_ensemble[model_idx, :, 0] = y_mean\n",
    "        predictions_of_ensemble[model_idx, :, 1] = y_var\n",
    "    return predictions_of_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a5d735",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9b0529",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join('data', 'yandex-shifts', 'weather')\n",
    "print(f'{data_dir} {os.path.isdir(data_dir)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79c0239",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = os.path.join('models', 'yandex-shifts', 'weather')\n",
    "print(f'{model_dir} {os.path.isdir(model_dir)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3708c2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainxdata_name = os.path.join(data_dir, 'x_train_cube.zarr')\n",
    "print(f'{trainxdata_name} {os.path.isfile(trainxdata_name)}')\n",
    "trainydata_name = os.path.join(data_dir, 'y_train_cube.zarr')\n",
    "print(f'{trainydata_name} {os.path.isfile(trainydata_name)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4b7ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_in_name = os.path.join(data_dir, 'x_val_cube.zarr')\n",
    "print(f'{dev_in_name} {os.path.isfile(dev_in_name)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cefabba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_out_name = os.path.join(data_dir, 'y_val_cube.zarr')\n",
    "print(f'{dev_out_name} {os.path.isfile(dev_out_name)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7929a2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_name = os.path.join(data_dir, 'x_test_cube.zarr')\n",
    "print(f'{eval_name} {os.path.isfile(eval_name)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15cdbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_x = xr.open_zarr(trainxdata_name).to_dataframe().dropna()#.to_numpy().astype(np.float64)\n",
    "df_train_y = xr.open_zarr(trainydata_name).to_dataframe().dropna()#.to_numpy().astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train =df_train_x.to_numpy().astype(np.float64)\n",
    "y_train= df_train_y.to_numpy().astype(np.float64)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_train= y_train[:,1:2]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_train = y_train.ravel()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xr.open_zarr(trainydata_name).to_dataframe().dropna()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eb7d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'X_train: dtype = {X_train.dtype}, shape = {X_train.shape}')\n",
    "print(f'y_train: dtype = {y_train.dtype}, shape = {y_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3149ca46",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_preprocessor = build_preprocessor(X_train, df_train_x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af307cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(common_preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2590fb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = common_preprocessor.transform(X_train)\n",
    "num_features = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc560d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_values = all(np.isfinite(X_train).ravel())\n",
    "if not correct_values:\n",
    "    raise ValueError('Some values of input values are not correct (NaNs or infinite)!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf19ab01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Maximal value of input matrix is {np.max(X_train)}.')\n",
    "print(f'Minimal value of input matrix is {np.min(X_train)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b981022",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_indices = list(range(X_train.shape[0]))\n",
    "random.shuffle(all_indices)\n",
    "X_train = X_train[all_indices]\n",
    "y_train = y_train[all_indices]\n",
    "del all_indices\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85691b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_for_projections = random.sample(\n",
    "    population=list(range(X_train.shape[0])),\n",
    "    k=100000\n",
    ")\n",
    "X_train_prj = reduce_dimensions_of_data(X_train[indices_for_projections])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffea1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_temperature(X_train_prj, y_train[indices_for_projections],\n",
    "                 figure_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b69393",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c591604e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_temperatures = sorted(y_train.tolist())\n",
    "min_temperature = all_temperatures[0]\n",
    "max_temperature = all_temperatures[-1]\n",
    "n_samples_in_trainset = len(all_temperatures)\n",
    "temperature_001 = all_temperatures[int(round(0.001 * n_samples_in_trainset))]\n",
    "temperature_999 = all_temperatures[int(round(0.999 * n_samples_in_trainset))]\n",
    "print(f'Minimal temperature is {min_temperature}.')\n",
    "print(f'Maximal temperature is {max_temperature}.')\n",
    "print(f'0.1% of temperature is {temperature_001}.')\n",
    "print(f'99.9% of temperature is {temperature_999}.')\n",
    "max_temperature = int(np.ceil(temperature_999))\n",
    "min_temperature = int(np.floor(temperature_001))\n",
    "n_classes = max_temperature - min_temperature + 1\n",
    "dict_of_classes = dict()\n",
    "for class_idx in range(n_classes):\n",
    "    dict_of_classes[min_temperature + class_idx] = class_idx\n",
    "print(f'Number of temperature classes is {n_classes}.')\n",
    "print('They are:')\n",
    "for temperature_val in dict_of_classes:\n",
    "    class_idx = dict_of_classes[temperature_val]\n",
    "    print('  Class {0:>2}: temperature = {1:4.1f}'.format(class_idx, temperature_val))\n",
    "y_train_class = np.empty(y_train.shape, dtype=np.int64)\n",
    "for sample_idx in range(y_train.shape[0]):\n",
    "    temperature_val = int((y_train[sample_idx]))\n",
    "    if temperature_val in dict_of_classes:\n",
    "        class_idx = dict_of_classes[temperature_val]\n",
    "    else:\n",
    "        if temperature_val < min_temperature:\n",
    "            class_idx = 0\n",
    "        else:\n",
    "            class_idx = n_classes - 1\n",
    "    y_train_class[sample_idx] = class_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57fc52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'X_train: dtype = {X_train.dtype}, shape = {X_train.shape}')\n",
    "print(f'y_train: dtype = {y_train.dtype}, shape = {y_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75b0d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n",
    "splitter = skf.split(X_train, y_train_class)\n",
    "splits = [(train_index, test_index) for train_index, test_index in splitter]\n",
    "del splitter, skf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ee2d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocessing_scalers = []\n",
    "deep_ensemble = []\n",
    "BATCH_SIZE = 4096\n",
    "MAX_EPOCHS = 1\n",
    "PATIENCE = 15\n",
    "new_figure_id = 5\n",
    "BEST_LAYER_SIZE = 512\n",
    "best_hyperparams = (18, 3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649faf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efda4875",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for train_index, test_index in splits:\n",
    "    model_name = f'weather_snn_{len(deep_ensemble) + 1}'\n",
    "    serialization_name = os.path.join(model_dir, model_name + '.h5')\n",
    "    regression_output_name = f'{model_name}_distribution'\n",
    "    projection_output_name = f'{model_name}_projection'\n",
    "    printable_name = f'Self-Normalizing Network ' \\\n",
    "                     f'{len(deep_ensemble) + 1} for weather prediction'\n",
    "    print('========================================')\n",
    "    print(' ' + printable_name)\n",
    "    print('========================================')\n",
    "    print('')\n",
    "    new_postprocessing_scaler = StandardScaler().fit(\n",
    "        y_train[train_index].reshape((len(train_index), 1))\n",
    "    )\n",
    "    X_train_ = X_train[train_index].astype(np.float32)\n",
    "    y_train_ = new_postprocessing_scaler.transform(\n",
    "        y_train[train_index].reshape((len(train_index), 1))\n",
    "    ).flatten().astype(np.float32)\n",
    "    y_train_class_ = y_train_class[train_index]\n",
    "    X_val_ = X_train[test_index].astype(np.float32)\n",
    "    y_val_ = y_train[test_index]\n",
    "    y_val_class_ = y_train_class[test_index]\n",
    "    y_val_scaled_ = new_postprocessing_scaler.transform(\n",
    "        y_val_.reshape((len(test_index), 1))\n",
    "    ).flatten().astype(np.float32)\n",
    "    steps_per_epoch = X_train_.shape[0] // BATCH_SIZE\n",
    "    postprocessing_scalers.append(new_postprocessing_scaler)\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            X_train_,\n",
    "            (\n",
    "                y_train_,\n",
    "                y_train_class_\n",
    "            )\n",
    "        )\n",
    "    ).repeat().shuffle(1000000).batch(BATCH_SIZE)\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            X_val_,\n",
    "            (\n",
    "                y_val_scaled_,\n",
    "                y_val_class_\n",
    "            )\n",
    "        )\n",
    "    ).batch(BATCH_SIZE)\n",
    "    del X_train_, y_train_, y_val_scaled_, y_train_class_, y_val_class_\n",
    "    new_model = build_neural_network(\n",
    "        input_size=num_features,\n",
    "        layer_size=BEST_LAYER_SIZE,\n",
    "        n_layers=best_hyperparams[0],\n",
    "        dropout_rate=best_hyperparams[1],\n",
    "        scale_coeff=new_postprocessing_scaler.scale_[0],\n",
    "        nn_name=model_name\n",
    "    )\n",
    "    new_model.summary()\n",
    "    print('')\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=f'val_{regression_output_name}_mean_absolute_error',\n",
    "            restore_best_weights=True,\n",
    "            patience=PATIENCE, verbose=True\n",
    "        )\n",
    "    ]\n",
    "    model_history = new_model.fit(\n",
    "        train_dataset, epochs=MAX_EPOCHS, steps_per_epoch=steps_per_epoch,\n",
    "        callbacks=callbacks, validation_data=val_dataset, verbose=1\n",
    "    )\n",
    "    new_model.save_weights(serialization_name, overwrite=True, save_format='h5')\n",
    "    del train_dataset, val_dataset\n",
    "    print('')\n",
    "    show_training_process(history=model_history, metric_name='loss',\n",
    "                          figure_id=new_figure_id, comment=printable_name)\n",
    "    new_figure_id += 1\n",
    "    show_training_process(history=model_history,\n",
    "                          metric_name=f'{regression_output_name}_loss',\n",
    "                          figure_id=new_figure_id, comment=printable_name)\n",
    "    new_figure_id += 1\n",
    "    show_training_process(history=model_history,\n",
    "                          metric_name=f'{regression_output_name}_mean_absolute_error',\n",
    "                          figure_id=new_figure_id, comment=printable_name)\n",
    "    new_figure_id += 1\n",
    "    del model_history, callbacks\n",
    "    print('')\n",
    "    deep_ensemble.append(new_model)\n",
    "    instant_res = predict_with_single_nn(X_val_, new_model, BATCH_SIZE,\n",
    "                                         output_scaler=new_postprocessing_scaler)\n",
    "    y_pred_mean = instant_res[0]\n",
    "    y_pred_var = instant_res[1]\n",
    "    rauc_mse_score = evaluate_single_nn(y_pred_mean, y_pred_var, y_val_)\n",
    "    print('Test quality:')\n",
    "    print(f'  mean absolute error   = {mean_absolute_error(y_val_, y_pred_mean)}')\n",
    "    print(f'  mean squared error    = {mean_squared_error(y_val_, y_pred_mean)}')\n",
    "    print(f'  median absolute error = {median_absolute_error(y_val_, y_pred_mean)}')\n",
    "    print(f'  r2 score              = {r2_score(y_val_, y_pred_mean)}')\n",
    "    print(f'  R-AUC MSE             = {rauc_mse_score}')\n",
    "    del X_val_, y_val_\n",
    "    del new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4cec46",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_name = os.path.join(model_dir, 'weather_snn_config.json')\n",
    "best_hyperparams_for_saving = {\n",
    "    'input_size': num_features,\n",
    "    'ensemble_size': len(splits),\n",
    "    'layer_size': BEST_LAYER_SIZE,\n",
    "    'n_layers': best_hyperparams[0],\n",
    "    'alpha_dropout_rate': best_hyperparams[1]\n",
    "}\n",
    "with codecs.open(config_name, mode='w', encoding='utf-8') as fp:\n",
    "    json.dump(best_hyperparams_for_saving, fp, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5797dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_name = os.path.join(model_dir, 'preprocessing_pipeline.pkl')\n",
    "with open(preprocessing_name, 'wb') as fp:\n",
    "    pickle.dump(common_preprocessor, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb0cf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocessing_name = os.path.join(model_dir, 'postprocessing_scalers.pkl')\n",
    "with open(postprocessing_name, 'wb') as fp:\n",
    "    pickle.dump(postprocessing_scalers, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e3cc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in = xr.open_zarr(dev_in_name).to_dataframe().dropna()#pd.read_csv(dev_in_name)\n",
    "print(f'Row number is {df_in.shape[0]}.')\n",
    "print(f'Column number is {df_in.shape[1]}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a24b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = xr.open_zarr(dev_out_name).to_dataframe().dropna()#pd.read_csv(dev_out_name)\n",
    "print(f'Row number is {df_out.shape[0]}.')\n",
    "print(f'Column number is {df_out.shape[1]}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb5c1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''inputs = np.vstack([\n",
    "    df_in.to_numpy().astype(np.float64),\n",
    "    df_out.to_numpy().astype(np.float64)\n",
    "])\n",
    "targets = np.concatenate([\n",
    "    df_in['fact_temperature'].to_numpy().astype(np.float64),\n",
    "    df_out['fact_temperature'].to_numpy().astype(np.float64)\n",
    "])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "inputs = df_in.to_numpy().astype(np.float64)\n",
    "targets = df_out.to_numpy().astype(np.float64)\n",
    "targets = targets[:, 1:2]\n",
    "targets = targets.ravel()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e31b32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = predict_by_ensemble(\n",
    "    input_data=inputs,\n",
    "    preprocessing=common_preprocessor,\n",
    "    postprocessing=postprocessing_scalers,\n",
    "    ensemble=deep_ensemble,\n",
    "    minibatch=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfd4bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_uncertainty = ensemble_uncertainties_regression(all_preds)\n",
    "uncertainties = all_uncertainty['tvar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378e78f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds_mean = all_preds[:,:,0]\n",
    "avg_preds = np.squeeze(np.mean(all_preds_mean, axis=0))\n",
    "errors = (avg_preds - targets) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b98342",
   "metadata": {},
   "outputs": [],
   "source": [
    "rejection_mse = calc_uncertainty_regection_curve(errors, uncertainties)\n",
    "retention_mse = rejection_mse[::-1]\n",
    "retention_fractions = np.linspace(0, 1, len(retention_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b16fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'R-AUC MSE = {np.mean(retention_mse)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7952cc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(new_figure_id, figsize=(7, 7))\n",
    "new_figure_id += 1\n",
    "plt.plot(retention_fractions, retention_mse)\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('Retention Fraction')\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d64c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 1.0\n",
    "f_auc, f95, retention_f1 = f_beta_metrics(errors, uncertainties, thresh, beta=1.0)\n",
    "print(f'F1 score at 95% retention: {f95}')\n",
    "retention_fractions = np.linspace(0, 1, len(retention_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a329ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(new_figure_id, figsize=(7, 7))\n",
    "new_figure_id += 1\n",
    "plt.plot(retention_fractions, retention_f1)\n",
    "plt.ylabel('F1')\n",
    "plt.xlabel('Retention Fraction')\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7c8cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = np.arange(1, inputs.shape[0] + 1)\n",
    "preds = np.mean(all_preds[:,:,0], axis=0)\n",
    "df_submission = pd.DataFrame(data={\n",
    "    'ID' : ids,\n",
    "    'PRED' : preds,\n",
    "    'UNCERTAINTY' : uncertainties\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25be5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e302e60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = os.path.join(model_dir, 'df_submission_dev.csv')\n",
    "df_submission.to_csv(out_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_eval= xr.open_zarr(eval_name).to_dataframe().dropna()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793f3ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_inputs = df_eval.to_numpy().astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c1f8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = predict_by_ensemble(\n",
    "    input_data=eval_inputs,\n",
    "    preprocessing=common_preprocessor,\n",
    "    postprocessing=postprocessing_scalers,\n",
    "    ensemble=deep_ensemble,\n",
    "    minibatch=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b0dbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_uncertainty = ensemble_uncertainties_regression(all_preds)\n",
    "uncertainties = all_uncertainty['tvar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02968078",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = np.arange(1, len(df_eval) + 1)\n",
    "preds = np.mean(all_preds[:,:,0], axis=0)\n",
    "df_submission = pd.DataFrame(data={\n",
    "    'ID' : ids,\n",
    "    'PRED' : preds,\n",
    "    'UNCERTAINTY' : uncertainties\n",
    "})\n",
    "df_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e320d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = os.path.join(model_dir, 'df_submission.csv')\n",
    "df_submission.to_csv(out_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}